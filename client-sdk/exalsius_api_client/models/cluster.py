# coding: utf-8

"""
exalsius API

The exalsius REST API enables programmatic access to GPU infrastructure management and orchestration capabilities. Access the API through the `exls` command-line tool or integrate it directly into your applications using standard HTTP requests. The API covers several areas: * **GPU Market Offers** Browse and compare GPU instance pricing across public cloud providers and hyperscalers. * **Operator Integration** Coordinates with the [exalsius-operator](https://github.com/exalsius/exalsius-operator) running in a management Kubernetes cluster to handle infrastructure provisioning and node lifecycle management. * **Node Management** Import cloud-provider instances or self-managed nodes (via SSH) into your node pool. Hardware characteristics of self-managed nodes are discovered automatically. * **Cluster Provisioning** Create and manage Kubernetes clusters on supported cloud providers or self-managed bare-metal infrastructure. * **Service Deployment** Deploy infrastructure services such as the NVIDIA GPU Operator, KubeRay, Flyte, or Kubeflow onto your clusters. * **Workspace Deployment** Provision application workloads including Jupyter Notebook servers, LLM inference services, and other compute workloads on your clusters.

The version of the OpenAPI document: 1.35.0
Contact: support@exalsius.ai
Generated by OpenAPI Generator (https://openapi-generator.tech)

Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from datetime import datetime
from typing import Any, ClassVar, Dict, List, Optional, Set, Union

from pydantic import (BaseModel, ConfigDict, Field, StrictBool, StrictFloat,
                      StrictInt, StrictStr, field_validator)
from typing_extensions import Self

from exalsius_api_client.models.cluster_local_storage import \
    ClusterLocalStorage
from exalsius_api_client.models.service_deployment import ServiceDeployment
from exalsius_api_client.models.workspace_deployment import WorkspaceDeployment


class Cluster(BaseModel):
    """
    Cluster
    """  # noqa: E501

    id: Optional[StrictStr] = Field(
        default=None, description="The unique identifier for the cluster"
    )
    colony_id: Optional[StrictStr] = Field(
        default=None,
        description="The unique identifier for the colony that the cluster belongs to",
    )
    name: StrictStr = Field(description="The name of the cluster")
    namespace: Optional[StrictStr] = Field(
        default=None, description="The namespace the cluster resides in"
    )
    owner: Optional[StrictStr] = Field(
        default=None, description="The owner of the cluster (user id)"
    )
    cluster_type: Optional[StrictStr] = Field(
        default=None,
        description="The type of the cluster. - `CLOUD`: Cloud cluster, consisting of cloud instances - `REMOTE`: Remote cluster, consisting of self-managed nodes - `ADOPTED`: Adopted cluster, consisting of an already existing kubernetes cluster - `DOCKER`: Docker cluster, consisting of docker containers (for local testing and development) ",
    )
    vpn_cluster: Optional[StrictBool] = Field(
        default=False, description="Whether the cluster is a VPN cluster"
    )
    telemetry_enabled: Optional[StrictBool] = Field(
        default=False,
        description="Whether telemetry data collection is enabled for the cluster",
    )
    cluster_status: StrictStr = Field(
        description="The status of the cluster. - `PENDING`: Cluster is pending (not yet deployed) - `DEPLOYING`: Cluster is being deployed - `READY`: Cluster is ready - `FAILED`: Cluster is failed "
    )
    cluster_labels: Optional[Dict[str, StrictStr]] = Field(
        default=None, description="The labels of the cluster (optional)."
    )
    machine_pre_start_commands: Optional[List[StrictStr]] = Field(
        default=None,
        description="The commands to run on the machine before the cluster is started",
    )
    local_storage: Optional[ClusterLocalStorage] = None
    created_at: datetime = Field(
        description="The date and time the cluster was created"
    )
    updated_at: Optional[datetime] = Field(
        default=None, description="The date and time the cluster was last updated"
    )
    to_be_deleted_at: Optional[datetime] = Field(
        default=None, description="The date and time the cluster will be deleted"
    )
    control_plane_node_ids: Optional[List[StrictStr]] = Field(
        default=None,
        description="The node IDs of the control plane nodes in the cluster",
    )
    worker_node_ids: Optional[List[StrictStr]] = Field(
        default=None, description="The node IDs of the worker nodes in the cluster"
    )
    service_deployments: Optional[List[ServiceDeployment]] = Field(
        default=None, description="The deployed services in the cluster"
    )
    workspace_deployments: Optional[List[WorkspaceDeployment]] = Field(
        default=None, description="The deployed workspaces in the cluster"
    )
    k8s_version: Optional[StrictStr] = Field(
        default=None, description="The version of Kubernetes deployed in the cluster"
    )
    current_costs: Optional[Union[StrictFloat, StrictInt]] = Field(
        default=None, description="The total costs of the cluster until now"
    )
    costs_per_hour: Optional[Union[StrictFloat, StrictInt]] = Field(
        default=None, description="The costs of the cluster per hour"
    )
    __properties: ClassVar[List[str]] = [
        "id",
        "colony_id",
        "name",
        "namespace",
        "owner",
        "cluster_type",
        "vpn_cluster",
        "telemetry_enabled",
        "cluster_status",
        "cluster_labels",
        "machine_pre_start_commands",
        "local_storage",
        "created_at",
        "updated_at",
        "to_be_deleted_at",
        "control_plane_node_ids",
        "worker_node_ids",
        "service_deployments",
        "workspace_deployments",
        "k8s_version",
        "current_costs",
        "costs_per_hour",
    ]

    @field_validator("cluster_type")
    def cluster_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(["CLOUD", "REMOTE", "ADOPTED", "DOCKER"]):
            raise ValueError(
                "must be one of enum values ('CLOUD', 'REMOTE', 'ADOPTED', 'DOCKER')"
            )
        return value

    @field_validator("cluster_status")
    def cluster_status_validate_enum(cls, value):
        """Validates the enum"""
        if value not in set(["PENDING", "DEPLOYING", "READY", "FAILED"]):
            raise ValueError(
                "must be one of enum values ('PENDING', 'DEPLOYING', 'READY', 'FAILED')"
            )
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of Cluster from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of local_storage
        if self.local_storage:
            _dict["local_storage"] = self.local_storage.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in service_deployments (list)
        _items = []
        if self.service_deployments:
            for _item_service_deployments in self.service_deployments:
                if _item_service_deployments:
                    _items.append(_item_service_deployments.to_dict())
            _dict["service_deployments"] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in workspace_deployments (list)
        _items = []
        if self.workspace_deployments:
            for _item_workspace_deployments in self.workspace_deployments:
                if _item_workspace_deployments:
                    _items.append(_item_workspace_deployments.to_dict())
            _dict["workspace_deployments"] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of Cluster from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "id": obj.get("id"),
                "colony_id": obj.get("colony_id"),
                "name": obj.get("name"),
                "namespace": obj.get("namespace"),
                "owner": obj.get("owner"),
                "cluster_type": obj.get("cluster_type"),
                "vpn_cluster": (
                    obj.get("vpn_cluster")
                    if obj.get("vpn_cluster") is not None
                    else False
                ),
                "telemetry_enabled": (
                    obj.get("telemetry_enabled")
                    if obj.get("telemetry_enabled") is not None
                    else False
                ),
                "cluster_status": obj.get("cluster_status"),
                "cluster_labels": obj.get("cluster_labels"),
                "machine_pre_start_commands": obj.get("machine_pre_start_commands"),
                "local_storage": (
                    ClusterLocalStorage.from_dict(obj["local_storage"])
                    if obj.get("local_storage") is not None
                    else None
                ),
                "created_at": obj.get("created_at"),
                "updated_at": obj.get("updated_at"),
                "to_be_deleted_at": obj.get("to_be_deleted_at"),
                "control_plane_node_ids": obj.get("control_plane_node_ids"),
                "worker_node_ids": obj.get("worker_node_ids"),
                "service_deployments": (
                    [
                        ServiceDeployment.from_dict(_item)
                        for _item in obj["service_deployments"]
                    ]
                    if obj.get("service_deployments") is not None
                    else None
                ),
                "workspace_deployments": (
                    [
                        WorkspaceDeployment.from_dict(_item)
                        for _item in obj["workspace_deployments"]
                    ]
                    if obj.get("workspace_deployments") is not None
                    else None
                ),
                "k8s_version": obj.get("k8s_version"),
                "current_costs": obj.get("current_costs"),
                "costs_per_hour": obj.get("costs_per_hour"),
            }
        )
        return _obj
